input {
{{- if .Values.kafka.enabled }}
  
  kafka {
    bootstrap_servers => ["${KAFKA_BOOTSTRAP_SERVERS}"]
    topics => ["containers"]
    client_id => "${HOSTNAME}-contaienrs"
    group_id => "containers"
    auto_offset_reset => "latest"
    consumer_threads => 10
  }

{{ else }}  

  beats {
    port => 5044
  }
    
{{- end }}

}

filter {
  #all messages from the kafka topics are in json format
  {{- if .Values.kafka.enabled }}
    json {
      skip_on_invalid_json => false
      source => "message"
    }
  {{- end }}

  #remove ANSI Color bytes
  mutate {
    gsub => ["message", "\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]", ""]
  }

  #all application json formated logs can be parsed
  # we parse the payload of the message field for json
  {{- if .Values.logstash_indexer.json_messages }}
    json {
      skip_on_invalid_json => false
      source => "message"
      add_tag => [ "json_parsed_successful" ]
    }
  {{- end }} 


  if [logEvent][message]  {
    mutate {
       replace => { "message" => "%{[logEvent][message]}"
         remove_field => [ "%{[logEvent]}" ]
       }
    }
  }

  #in some cases we have msg instead of message holding the log
  if [msg] {
    mutate {
        replace => { "message" => "%{msg}" }
        remove_field => ["msg"]
    }
  }

  #some logs keep timestamps in ts field
  if [ts] {
    date {
        match => [ "ts", "MMM dd, yyyy @ HH:mm:ss.SSS", "ISO8601" ]
        target => "@timestamp"
        tag_on_failure => ["_dateparsefailure"]
    }
    mutate {
      remove_field => ["ts"]
    }
  }

  #some logs keep timestamps in timestamp field
  if [timestamp] {
    date {
        match => [ "timestamp", "MMM dd, yyyy @ HH:mm:ss.SSS", "ISO8601" ]
        target => "@timestamp"
        tag_on_failure => ["_dateparsefailure"]
    }
    mutate {
      remove_field => ["timestamp"]
    }
  }  


  {{- if .Values.logstash_indexer.code_snippet.enabeld }}
  ruby {
    code => '
      {{- .Values.logstash_indexer.code_snippet.code -}} 
    '
  }
  {{- end }}
 
}

output {

  if "filebeat" in [kubernetes][labels][k8s-app]  {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST}"]
      user => "${ELASTICSEARCH_USER:\"\"}"
      password => "${ELASTICSEARCH_PASSWORD:\"\"}"
      index => "filebeats-%{+YYYY.MM.dd}"
      cacert => "/usr/share/logstash/config/${CA_CERT}"
      ssl => "true"
      ssl_certificate_verification => "false"
      manage_template => true
      template => "/usr/share/logstash/config/filebeats_template.json"
      template_name => "filebeats_1"
      template_overwrite => true
      ilm_enabled => "false"
      document_id => "%{[@metadata][_id]}"
    }
  } else {
  
  # all other containers stdout and stderr go to container ES index
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST}"]
      user => "${ELASTICSEARCH_USER:\"\"}"
      password => "${ELASTICSEARCH_PASSWORD:\"\"}"
      index => "containers-%{+YYYY.MM.dd}"
      cacert => "/usr/share/logstash/config/${CA_CERT}"
      ssl => "true"
      ssl_certificate_verification => "false"
      manage_template => true
      template => "/usr/share/logstash/config/containers_template.json"
      template_name => "containers_1"
      template_overwrite => true
      ilm_enabled => "false"
      document_id => "%{[@metadata][_id]}"
    }
  }
}
