# Default values for kubernetes-logging.
# Global values
cluster_name: "logging"
imagePullSecrets: []
storage_class: "standard"

#Opendistro configuration 
opendistro:
  image: "amazon/opendistro-for-elasticsearch"
  imageTag: "1.12.0"
  saml:
    enabled: false
#    idp:
#      metadata_url:
#      entity_id:
#    sp:
#      entity_id:
#    exchange_key:
#    admin_role:
#    viewer_role:  
#    tenant_role:  

#ES Curator job configuration
es_curator_image:
  image: "nickytd/es-curator"
  imageTag: "5.8"

#Init container configuration. Used for multiple application startup checks
init_container_image:
  image: "nickytd/init-container"
  imageTag: "latest"
  imagePullPolicy: IfNotPresent

#ES configuration
#A complete ES setup is provisioned when "in_cluster" is set to true. It can be scaled accordingly to the environment needs
#with the concrete configurations of the nodes that follow.
#In case "in_cluster" is set to false, the logstash-indexer uses it as an output destination. The coordination, master and data nodes
#are skiped.
elasticsearch:
  image: "docker.elastic.co"
  imageTag: "7.10.0"
  in_cluster: true
  with_node_roles: true
  snapshot:
    enabled: false
    storage_class: ""
    size: "5Gi"
  index_shards: 1
  index_replicas: 0
  retention_days: 1
  additional_jvm_params: "-Djava.net.preferIPv4Stack=true -XshowSettings:properties -XshowSettings:vm -XshowSettings:system"
  url: ""
  user: "esadmin"
  password: "esadmin"

#Configuration of ES master node if "in_cluster" is true
master:
  minimum_nodes: 1
  replicas: 1
  storage: "1Gi"
  heap_size: "256m"
  resources:
    requests:
      memory: "600Mi"
    limits:
      memory: "600Mi"
  storage_class: ""    
  tolerations: []
  affinity: {}    

#Configuration of ES coordination node if "in_cluster" is true
client:
  replicas: 1
  heap_size: "512m"
  resources:
    requests:
      memory: "800Mi"
    limits:
      memory: "800Mi"
  host:
  ingress:
    path: "/"
    secret_name: ""
    enabled: false
    annotations: {}
  tolerations: []
  affinity: {}

#Configuration of ES data node if "in_cluster" is true
data:
  replicas: 1
  heap_size: "256m"
  storage: "1Gi"
  resources:
    requests:
      memory: "600Mi"
    limits:
      memory: "600Mi"
  storage_class: ""    
  tolerations: []
  affinity: {} 

#when in_cluster is set to false it determines an external kibana instance. 
#In this case only jobs creating index templates and kibana objects are executed.
kibana:
  in_cluster: true
  url: ""
  user: "kibana"
  password: "kibana"
  readonly:
     user: "viewer"
     password: "view"
  developer:
     user: "developer"
     password: "develop"   
  replicas: 1
  service_type: ClusterIP
  ingress:
    path: "/"
    enabled: false
    annotations: {}
  index: "containers,filebeats,journals,all"
  resources:
    requests:
      memory: "500Mi"
    limits:
      memory: "500Mi"
  tolerations: []
  affinity: {}    

#Logstash indexer is a target of various logging streams and can perform document routing decisions
#based on the documents content. Example the logs from journals stream are directed to "journal-<date>" indices.
#The logs from containers stdout and stderr are routed to "container-<date>" indices in ES.
#
#Json formated log messages are additionaly parsed when "json_messages" is set to true. In this case if an container
#output is json formatted it will be parsed. There could be a problem if two different containers logs use the same key
#wherer values are mapped to different ES field types.
logstash_indexer:
  json_messages: true
  ca_cert: "root-ca.pem"
  replicas: 1
  heap_size: "512m"
  resources:
    requests:
      memory: "800Mi"
    limits:
      memory: "800Mi"
  tolerations: []
  affinity: {}
  code_snippet: 
    enabled: false
    code: |-
      # code snippet for ruby filter plugin
      # https://www.elastic.co/guide/en/logstash/current/plugins-filters-ruby.html

#ES filebeats are used to collect containers stdout and stderror. 
#The workload is running as a deamon set on each node with the appropriate tollerations definitions
filebeat:
  logging:
    level: "warning"
    json: true
    metrics:
      enabled: false
  queue_size: 1024
  containers_host_path: "/var/lib/docker/containers"
  resources:
    limits: 
      memory: "100Mi"
  tolerations: []
  affinity: {}

#ES journalbeats are used to collect system logs from OS journalctl
#It depends on the concrete minion OS
#Those logs help to catch states related to OS services such as container runtimes and kubelet services
journalbeat:
  logging:
    level: "warning"
    json: true
    metrics:
      enabled: false
  resources:
    limits: 
      memory: "100Mi"    
  enabled: true
#needs adaptation for minikube case
# host_path: /var/log/journal 
  journals_host_path: "/var/run/log/journal"
  tolerations: []
  affinity: {}  

#In scaled out setup kafka queues are used as ingestion points to accomodate spiked in the logging stream volumes
kafka:
  enabled: true
  replicas: 1
  image: "wurstmeister/kafka"
  imageTag: "2.13-2.6.0"
  heap_size: "256m"
  storage: "1Gi"
  topics:
    config: "retention.bytes=134217728,retention.ms=3600000,message.timestamp.difference.max.ms=3600000,message.timestamp.type=LogAppendTime"
    name: ["containers","journals"]
    replication_factor: 0
    partitions: 1
  resources:
    requests:
      memory: "600Mi"
    limits:
      memory: "600Mi"
  storage_class: ""    
  tolerations: []
  affinity: {}   

#Zookeeper is a dependecy of kafka
zookeeper:
  replicas: 1
  image: "zookeeper"
  imageTag: "3.6.2"
  heap_size: "128m"
  storage: 1Gi
  resources:
    requests:
      memory: 300Mi
    limits:
      memory: 300Mi
  storage_class: ""
  tolerations: []
  affinity: {}